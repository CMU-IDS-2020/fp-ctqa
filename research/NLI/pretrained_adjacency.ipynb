{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pretrained-adjacency.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jGuJWHhCJ8L",
        "outputId": "cb97ef70-8591-41f0-f4da-1136fed494b6"
      },
      "source": [
        "import time\n",
        "import os\n",
        "import numpy as np\n",
        "import google.colab as colab\n",
        "import random\n",
        "import json\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from multiprocessing import Pool\n",
        "import shutil\n",
        "from pprint import pprint\n",
        "import pickle\n",
        "from random import randint\n",
        "import pandas as pd\n",
        "\n",
        "import re\n",
        "import inspect\n",
        "import torch\n",
        "from torch import optim\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfs2-AL8JD0i"
      },
      "source": [
        "# Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xykwbqUrJJUg"
      },
      "source": [
        "def mount_google_drive():\n",
        "\t'''\n",
        "\t# Functionality\n",
        "\t\tMount google drive. Since colab does not save files, we want to make it easier to directly access files in google drive.\n",
        "\t# Arguments\n",
        "\t\tNothing\n",
        "\t# Returns\n",
        "\t\tdrive_root: the working directory mounted\n",
        "\t'''\n",
        "\tmount_directory = \"/content/gdrive\"\n",
        "\tdrive = colab.drive\n",
        "\tdrive.mount(mount_directory, force_remount=True)\n",
        "\tdrive_root = mount_directory + \"/\" + list(filter(lambda x: x[0] != '.', os.listdir(mount_directory)))[0]\n",
        "\treturn drive_root"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hq7hUZxgJK7S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "622addf5-2db6-48d2-9afb-c3d24ca86947"
      },
      "source": [
        "# Please Set up mounted directories here. Notice whether you want to balance dataset\n",
        "ROOT_DIR =  mount_google_drive() + \"/05839-Final-Project/code/\"\n",
        "\n",
        "DATASET_PATH = ROOT_DIR + \"quora.csv\"\n",
        "\n",
        "NLI_NET_DIR = ROOT_DIR + \"models/NliNetUtils/\"\n",
        "\n",
        "CHECKPOINT_DIR = ROOT_DIR + \"checkpoints/\""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcSJYLrLNL9F"
      },
      "source": [
        "# Migrate utils from drive to current dir so that we don't need to upload a folder from local every time\n",
        "shutil.rmtree('utils/', ignore_errors=True)\n",
        "_ = shutil.copytree(ROOT_DIR +\"/utils/\", \"utils/\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2j5FQDGmKQ54"
      },
      "source": [
        "# Load custimizable utils here\n",
        "from utils.file_utils import *\n",
        "from utils.image_utils import *\n",
        "from utils.generator_utils import *\n",
        "from utils.tqdm_utils import *\n",
        "from utils.keras_utils import *"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEXNCE090hAn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7b81ed39-a455-4bf2-8747-a534a0a91d99"
      },
      "source": [
        "# Load infersent model related files\n",
        "shutil.rmtree('models.py', ignore_errors=True)\n",
        "shutil.copy(NLI_NET_DIR + \"models.py\", \"models.py\")\n",
        "\n",
        "shutil.rmtree('data.py', ignore_errors=True)\n",
        "shutil.copy(NLI_NET_DIR + \"data.py\", \"data.py\")\n",
        "\n",
        "shutil.rmtree('mutils.py', ignore_errors=True)\n",
        "shutil.copy(NLI_NET_DIR + \"mutils.py\", \"mutils.py\")\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'mutils.py'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Evmmh-AigeML"
      },
      "source": [
        "# shutil.rmtree('fastText/', ignore_errors=True)\n",
        "# shutil.copytree(ROOT_DIR + \"fastText/\", \"fastText/\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDrpU37uJR3g"
      },
      "source": [
        "from data import get_nli, get_batch, build_vocab\n",
        "from mutils import get_optimizer\n",
        "from models import NLINet"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "paLUu7Y5HToJ"
      },
      "source": [
        "def get_optimizer(s):\n",
        "    \"\"\"\n",
        "    Parse optimizer parameters.\n",
        "    Input should be of the form:\n",
        "        - \"sgd,lr=0.01\"\n",
        "        - \"adagrad,lr=0.1,lr_decay=0.05\"\n",
        "    \"\"\"\n",
        "    if \",\" in s:\n",
        "        method = s[:s.find(',')]\n",
        "        optim_params = {}\n",
        "        for x in s[s.find(',') + 1:].split(','):\n",
        "            split = x.split('=')\n",
        "            assert len(split) == 2\n",
        "            assert re.match(\"^[+-]?(\\d+(\\.\\d*)?|\\.\\d+)$\", split[1]) is not None\n",
        "            optim_params[split[0]] = float(split[1])\n",
        "    else:\n",
        "        method = s\n",
        "        optim_params = {}\n",
        "\n",
        "    if method == 'adadelta':\n",
        "        optim_fn = optim.Adadelta\n",
        "    elif method == 'adagrad':\n",
        "        optim_fn = optim.Adagrad\n",
        "    elif method == 'adam':\n",
        "        optim_fn = optim.Adam\n",
        "    elif method == 'adamax':\n",
        "        optim_fn = optim.Adamax\n",
        "    elif method == 'asgd':\n",
        "        optim_fn = optim.ASGD\n",
        "    elif method == 'rmsprop':\n",
        "        optim_fn = optim.RMSprop\n",
        "    elif method == 'rprop':\n",
        "        optim_fn = optim.Rprop\n",
        "    elif method == 'sgd':\n",
        "        optim_fn = optim.SGD\n",
        "        assert 'lr' in optim_params\n",
        "    else:\n",
        "        raise Exception('Unknown optimization method: \"%s\"' % method)\n",
        "\n",
        "    # check that we give good parameters to the optimizer\n",
        "    expected_args = inspect.getargspec(optim_fn.__init__)[0]\n",
        "    assert expected_args[:2] == ['self', 'params']\n",
        "    if not all(k in expected_args[2:] for k in optim_params.keys()):\n",
        "        raise Exception('Unexpected parameters: expected \"%s\", got \"%s\"' % (\n",
        "            str(expected_args[2:]), str(optim_params.keys())))\n",
        "\n",
        "    return optim_fn, optim_params"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMwTFPfVZ6G1",
        "outputId": "76d08fb8-46f9-4a52-9260-a9929aa8c4b5"
      },
      "source": [
        "torch.cuda.is_available()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWnHB024C6oS"
      },
      "source": [
        "class InferSent(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(InferSent, self).__init__()\n",
        "        self.bsize = config['bsize']\n",
        "        self.word_emb_dim = config['word_emb_dim']\n",
        "        self.enc_lstm_dim = config['enc_lstm_dim']\n",
        "        self.pool_type = config['pool_type']\n",
        "        self.dpout_model = config['dpout_model']\n",
        "        self.version = 1 if 'version' not in config else config['version']\n",
        "\n",
        "        self.enc_lstm = nn.LSTM(self.word_emb_dim, self.enc_lstm_dim, 1,\n",
        "                                bidirectional=True, dropout=self.dpout_model)\n",
        "\n",
        "        assert self.version in [1, 2]\n",
        "        if self.version == 1:\n",
        "            self.bos = '<s>'\n",
        "            self.eos = '</s>'\n",
        "            self.max_pad = True\n",
        "            self.moses_tok = False\n",
        "        elif self.version == 2:\n",
        "            self.bos = '<p>'\n",
        "            self.eos = '</p>'\n",
        "            self.max_pad = False\n",
        "            self.moses_tok = True\n",
        "\n",
        "    def is_cuda(self):\n",
        "        # either all weights are on cpu or they are on gpu\n",
        "        return self.enc_lstm.bias_hh_l0.data.is_cuda\n",
        "\n",
        "    def forward(self, sent_tuple):\n",
        "        # sent_len: [max_len, ..., min_len] (bsize)\n",
        "        # sent: (seqlen x bsize x worddim)\n",
        "        sent, sent_len = sent_tuple\n",
        "\n",
        "        # Sort by length (keep idx)\n",
        "        sent_len_sorted, idx_sort = np.sort(sent_len)[::-1], np.argsort(-sent_len)\n",
        "        sent_len_sorted = sent_len_sorted.copy()\n",
        "        idx_unsort = np.argsort(idx_sort)\n",
        "\n",
        "        idx_sort = torch.from_numpy(idx_sort).cuda() if self.is_cuda() \\\n",
        "            else torch.from_numpy(idx_sort)\n",
        "        sent = sent.index_select(1, idx_sort)\n",
        "\n",
        "        # Handling padding in Recurrent Networks\n",
        "        sent_packed = nn.utils.rnn.pack_padded_sequence(sent, sent_len_sorted)\n",
        "        sent_output = self.enc_lstm(sent_packed)[0]  # seqlen x batch x 2*nhid\n",
        "        sent_output = nn.utils.rnn.pad_packed_sequence(sent_output)[0]\n",
        "\n",
        "        # Un-sort by length\n",
        "        idx_unsort = torch.from_numpy(idx_unsort).cuda() if self.is_cuda() \\\n",
        "            else torch.from_numpy(idx_unsort)\n",
        "        sent_output = sent_output.index_select(1, idx_unsort)\n",
        "\n",
        "        # Pooling\n",
        "        if self.pool_type == \"mean\":\n",
        "            sent_len = torch.FloatTensor(sent_len.copy()).unsqueeze(1).cuda()\n",
        "            emb = torch.sum(sent_output, 0).squeeze(0)\n",
        "            emb = emb / sent_len.expand_as(emb)\n",
        "        elif self.pool_type == \"max\":\n",
        "            if not self.max_pad:\n",
        "                sent_output[sent_output == 0] = -1e9\n",
        "            emb = torch.max(sent_output, 0)[0]\n",
        "            if emb.ndimension() == 3:\n",
        "                emb = emb.squeeze(0)\n",
        "                assert emb.ndimension() == 2\n",
        "\n",
        "        return emb\n",
        "\n",
        "    def set_w2v_path(self, w2v_path):\n",
        "        self.w2v_path = w2v_path\n",
        "\n",
        "    def get_word_dict(self, sentences, tokenize=True):\n",
        "        # create vocab of words\n",
        "        word_dict = {}\n",
        "        sentences = [s.split() if not tokenize else self.tokenize(s) for s in sentences]\n",
        "        for sent in sentences:\n",
        "            for word in sent:\n",
        "                if word not in word_dict:\n",
        "                    word_dict[word] = ''\n",
        "        word_dict[self.bos] = ''\n",
        "        word_dict[self.eos] = ''\n",
        "        return word_dict\n",
        "\n",
        "    def get_w2v(self, word_dict):\n",
        "        assert hasattr(self, 'w2v_path'), 'w2v path not set'\n",
        "        # create word_vec with w2v vectors\n",
        "        word_vec = {}\n",
        "        with open(self.w2v_path, encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                word, vec = line.split(' ', 1)\n",
        "                if word in word_dict:\n",
        "                    word_vec[word] = np.fromstring(vec, sep=' ')\n",
        "        print('Found %s(/%s) words with w2v vectors' % (len(word_vec), len(word_dict)))\n",
        "        return word_vec\n",
        "\n",
        "    def get_w2v_k(self, K):\n",
        "        assert hasattr(self, 'w2v_path'), 'w2v path not set'\n",
        "        # create word_vec with k first w2v vectors\n",
        "        k = 0\n",
        "        word_vec = {}\n",
        "        with open(self.w2v_path, encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                word, vec = line.split(' ', 1)\n",
        "                if k <= K:\n",
        "                    word_vec[word] = np.fromstring(vec, sep=' ')\n",
        "                    k += 1\n",
        "                if k > K:\n",
        "                    if word in [self.bos, self.eos]:\n",
        "                        word_vec[word] = np.fromstring(vec, sep=' ')\n",
        "\n",
        "                if k > K and all([w in word_vec for w in [self.bos, self.eos]]):\n",
        "                    break\n",
        "        return word_vec\n",
        "\n",
        "    def build_vocab(self, sentences, tokenize=True):\n",
        "        assert hasattr(self, 'w2v_path'), 'w2v path not set'\n",
        "        word_dict = self.get_word_dict(sentences, tokenize)\n",
        "        self.word_vec = self.get_w2v(word_dict)\n",
        "        print('Vocab size : %s' % (len(self.word_vec)))\n",
        "\n",
        "    # build w2v vocab with k most frequent words\n",
        "    def build_vocab_k_words(self, K):\n",
        "        assert hasattr(self, 'w2v_path'), 'w2v path not set'\n",
        "        self.word_vec = self.get_w2v_k(K)\n",
        "        print('Vocab size : %s' % (K))\n",
        "\n",
        "    def update_vocab(self, sentences, tokenize=True):\n",
        "        assert hasattr(self, 'w2v_path'), 'warning : w2v path not set'\n",
        "        assert hasattr(self, 'word_vec'), 'build_vocab before updating it'\n",
        "        word_dict = self.get_word_dict(sentences, tokenize)\n",
        "\n",
        "        # keep only new words\n",
        "        for word in self.word_vec:\n",
        "            if word in word_dict:\n",
        "                del word_dict[word]\n",
        "\n",
        "        # udpate vocabulary\n",
        "        if word_dict:\n",
        "            new_word_vec = self.get_w2v(word_dict)\n",
        "            self.word_vec.update(new_word_vec)\n",
        "        else:\n",
        "            new_word_vec = []\n",
        "        print('New vocab size : %s (added %s words)'% (len(self.word_vec), len(new_word_vec)))\n",
        "\n",
        "    def get_batch(self, batch):\n",
        "        # sent in batch in decreasing order of lengths\n",
        "        # batch: (bsize, max_len, word_dim)\n",
        "        embed = np.zeros((len(batch[0]), len(batch), self.word_emb_dim))\n",
        "\n",
        "        for i in range(len(batch)):\n",
        "            for j in range(len(batch[i])):\n",
        "                embed[j, i, :] = self.word_vec[batch[i][j]]\n",
        "\n",
        "        return torch.FloatTensor(embed)\n",
        "\n",
        "    def tokenize(self, s):\n",
        "        from nltk.tokenize import word_tokenize\n",
        "        if self.moses_tok:\n",
        "            s = ' '.join(word_tokenize(s))\n",
        "            s = s.replace(\" n't \", \"n 't \")  # HACK to get ~MOSES tokenization\n",
        "            return s.split()\n",
        "        else:\n",
        "            return word_tokenize(s)\n",
        "\n",
        "    def prepare_samples(self, sentences, bsize, tokenize, verbose):\n",
        "        sentences = [[self.bos] + s.split() + [self.eos] if not tokenize else\n",
        "                     [self.bos] + self.tokenize(s) + [self.eos] for s in sentences]\n",
        "        n_w = np.sum([len(x) for x in sentences])\n",
        "\n",
        "        # filters words without w2v vectors\n",
        "        for i in range(len(sentences)):\n",
        "            s_f = [word for word in sentences[i] if word in self.word_vec]\n",
        "            if not s_f:\n",
        "                import warnings\n",
        "                warnings.warn('No words in \"%s\" (idx=%s) have w2v vectors. \\\n",
        "                               Replacing by \"</s>\"..' % (sentences[i], i))\n",
        "                s_f = [self.eos]\n",
        "            sentences[i] = s_f\n",
        "\n",
        "        lengths = np.array([len(s) for s in sentences])\n",
        "        n_wk = np.sum(lengths)\n",
        "        if verbose:\n",
        "            print('Nb words kept : %s/%s (%.1f%s)' % (\n",
        "                        n_wk, n_w, 100.0 * n_wk / n_w, '%'))\n",
        "\n",
        "        # sort by decreasing length\n",
        "        lengths, idx_sort = np.sort(lengths)[::-1], np.argsort(-lengths)\n",
        "        sentences = np.array(sentences)[idx_sort]\n",
        "\n",
        "        return sentences, lengths, idx_sort\n",
        "\n",
        "    def encode(self, sentences, bsize=64, tokenize=True, verbose=False):\n",
        "        tic = time.time()\n",
        "        sentences, lengths, idx_sort = self.prepare_samples(\n",
        "                        sentences, bsize, tokenize, verbose)\n",
        "\n",
        "        embeddings = []\n",
        "        for stidx in range(0, len(sentences), bsize):\n",
        "            batch = self.get_batch(sentences[stidx:stidx + bsize])\n",
        "            if self.is_cuda():\n",
        "                batch = batch.cuda()\n",
        "            with torch.no_grad():\n",
        "                batch = self.forward((batch, lengths[stidx:stidx + bsize])).data.cpu().numpy()\n",
        "            embeddings.append(batch)\n",
        "        embeddings = np.vstack(embeddings)\n",
        "\n",
        "        # unsort\n",
        "        idx_unsort = np.argsort(idx_sort)\n",
        "        embeddings = embeddings[idx_unsort]\n",
        "\n",
        "        if verbose:\n",
        "            print('Speed : %.1f sentences/s (%s mode, bsize=%s)' % (\n",
        "                    len(embeddings)/(time.time()-tic),\n",
        "                    'gpu' if self.is_cuda() else 'cpu', bsize))\n",
        "        return embeddings\n",
        "\n",
        "    def visualize(self, sent, tokenize=True):\n",
        "\n",
        "        sent = sent.split() if not tokenize else self.tokenize(sent)\n",
        "        sent = [[self.bos] + [word for word in sent if word in self.word_vec] + [self.eos]]\n",
        "\n",
        "        if ' '.join(sent[0]) == '%s %s' % (self.bos, self.eos):\n",
        "            import warnings\n",
        "            warnings.warn('No words in \"%s\" have w2v vectors. Replacing \\\n",
        "                           by \"%s %s\"..' % (sent, self.bos, self.eos))\n",
        "        batch = self.get_batch(sent)\n",
        "\n",
        "        if self.is_cuda():\n",
        "            batch = batch.cuda()\n",
        "        output = self.enc_lstm(batch)[0]\n",
        "        output, idxs = torch.max(output, 0)\n",
        "        # output, idxs = output.squeeze(), idxs.squeeze()\n",
        "        idxs = idxs.data.cpu().numpy()\n",
        "        argmaxs = [np.sum((idxs == k)) for k in range(len(sent[0]))]\n",
        "\n",
        "        # visualize model\n",
        "        import matplotlib.pyplot as plt\n",
        "        x = range(len(sent[0]))\n",
        "        y = [100.0 * n / np.sum(argmaxs) for n in argmaxs]\n",
        "        plt.xticks(x, sent[0], rotation=45)\n",
        "        plt.bar(x, y)\n",
        "        plt.ylabel('%')\n",
        "        plt.title('Visualisation of words importance')\n",
        "        plt.show()\n",
        "\n",
        "        return output, idxs"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUq5W5A1C8z0",
        "outputId": "a66a7523-ac94-4688-becc-f150dbc00908"
      },
      "source": [
        "# !mkdir fastText\n",
        "# !curl -Lo fastText/crawl-300d-2M.vec.zip https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip\n",
        "# !unzip fastText/crawl-300d-2M.vec.zip -d fastText/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 1453M  100 1453M    0     0  22.9M      0  0:01:03  0:01:03 --:--:-- 22.8M\n",
            "Archive:  fastText/crawl-300d-2M.vec.zip\n",
            "  inflating: fastText/crawl-300d-2M.vec  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gobG5yFNulB",
        "outputId": "592f595c-15e1-48b6-a512-78d1250a3688"
      },
      "source": [
        "!mkdir encoder\n",
        "!curl -Lo encoder/infersent2.pkl https://dl.fbaipublicfiles.com/infersent/infersent2.pkl"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  146M  100  146M    0     0  19.9M      0  0:00:07  0:00:07 --:--:-- 23.3M\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6r_4BWjDOGU"
      },
      "source": [
        "def build_nli_net():\n",
        "  V = 2\n",
        "  MODEL_PATH = 'encoder/infersent%s.pkl' % V\n",
        "  params_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,\n",
        "                  'pool_type': 'max', 'dpout_model': 0.0, 'version': V}\n",
        "  infersent = InferSent(params_model)\n",
        "  infersent.load_state_dict(torch.load(MODEL_PATH))\n",
        "  return infersent"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "al33yHLrDR2Z"
      },
      "source": [
        "infersent = build_nli_net()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuvKcywGDSQi"
      },
      "source": [
        "W2V_PATH = 'fastText/crawl-300d-2M.vec'\n",
        "infersent.set_w2v_path(W2V_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYxkVTNSDUCj",
        "outputId": "c5642eaf-9408-43db-debd-b5ad5763a875"
      },
      "source": [
        "infersent.build_vocab_k_words(K=500000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab size : 500000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSvvyNxnDquG"
      },
      "source": [
        "def text_prepare(text):\n",
        "    REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;#]')\n",
        "    BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
        "    text = str(text)\n",
        "    # text = \" \".join([word for word in text.split(\" \") if re.search('[a-zA-Z]', word)])\n",
        "    # text = text.lower()\n",
        "    # text = re.sub(REPLACE_BY_SPACE_RE, \" \", text)\n",
        "    # text = re.sub(BAD_SYMBOLS_RE, \"\", text)\n",
        "    return text\n",
        "\n",
        "def cosine(u, v):\n",
        "  # compute the similarity between two embeddings\n",
        "  # u and v are matrices!\n",
        "    result = np.einsum('ij,ij->i', u, v) / ((np.linalg.norm(u, axis=1) * np.linalg.norm(v, axis=1)))\n",
        "    return np.log(result) + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghxPfXp5DYwa",
        "outputId": "bda1ce82-a75c-4ccc-e75d-00704ad33908"
      },
      "source": [
        "tweet_1 = \"Since the start of the pandemic, a total 65 WHO staff stationed in Geneva - working from home and onsite - have tested positive for #COVID19. We have not yet established whether any transmission has occurred on campus, but are looking into the matter.\"\n",
        "tweet_2 = \"WHO staff who were confirmed positive with #COVID19 in Geneva have received the necessary medical attention. WHO carried out full contact tracing and related protocols. Enhanced cleaning protocols were implemented in relevant offices.\"\n",
        "tweet_3 = \"Any tweets only my own views. More Guns,Less Crime (Univ Chicago Press, 3rd ed);10 books, 100+academic articles. PhD Econ, Advisor for Research & Science #USDOJ\"\n",
        "\n",
        "print(\"The similarity score between premise and hypoetheis 1 is:\")\n",
        "print(cosine(infersent.encode([text_prepare(tweet_1)]), infersent.encode([text_prepare(tweet_2)])).tolist()[0])\n",
        "print(\"The similarity score between premise and hypoetheis 2 is:\")\n",
        "print(cosine(infersent.encode([text_prepare(tweet_1)]), infersent.encode([text_prepare(tweet_3)])).tolist()[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The similarity score between premise and hypoetheis 1 is:\n",
            "0.6036133766174316\n",
            "The similarity score between premise and hypoetheis 2 is:\n",
            "0.5492755174636841\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWRo1K9AXdtr"
      },
      "source": [
        "## Look at twitter data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNDrnL63XfZc"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05DvmkGUXh46"
      },
      "source": [
        "df = pd.read_csv(\"tweets.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "RY6fxMQuXmHr",
        "outputId": "4bb7c18e-7241-4628-d496-4fc22f3b140c"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>coordinates</th>\n",
              "      <th>created_at</th>\n",
              "      <th>hashtags</th>\n",
              "      <th>media</th>\n",
              "      <th>urls</th>\n",
              "      <th>favorite_count</th>\n",
              "      <th>id</th>\n",
              "      <th>in_reply_to_screen_name</th>\n",
              "      <th>in_reply_to_status_id</th>\n",
              "      <th>in_reply_to_user_id</th>\n",
              "      <th>lang</th>\n",
              "      <th>place</th>\n",
              "      <th>possibly_sensitive</th>\n",
              "      <th>retweet_count</th>\n",
              "      <th>retweet_id</th>\n",
              "      <th>retweet_screen_name</th>\n",
              "      <th>source</th>\n",
              "      <th>text</th>\n",
              "      <th>tweet_url</th>\n",
              "      <th>user_created_at</th>\n",
              "      <th>user_screen_name</th>\n",
              "      <th>user_default_profile_image</th>\n",
              "      <th>user_description</th>\n",
              "      <th>user_favourites_count</th>\n",
              "      <th>user_followers_count</th>\n",
              "      <th>user_friends_count</th>\n",
              "      <th>user_listed_count</th>\n",
              "      <th>user_location</th>\n",
              "      <th>user_name</th>\n",
              "      <th>user_screen_name.1</th>\n",
              "      <th>user_statuses_count</th>\n",
              "      <th>user_time_zone</th>\n",
              "      <th>user_urls</th>\n",
              "      <th>user_verified</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>Sun Nov 01 03:52:22 +0000 2020</td>\n",
              "      <td>SootinClaimon</td>\n",
              "      <td>NaN</td>\n",
              "      <td>http://sootinclaimon.com/2020/11/01/the-corona...</td>\n",
              "      <td>0</td>\n",
              "      <td>1322748291646476288</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>en</td>\n",
              "      <td>NaN</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>&lt;a href=\"http://publicize.wp.com/\" rel=\"nofoll...</td>\n",
              "      <td>The coronavirus keeps most London theaters dar...</td>\n",
              "      <td>https://twitter.com/SoilFertilizer/status/1322...</td>\n",
              "      <td>Mon Oct 12 06:58:23 +0000 2009</td>\n",
              "      <td>SoilFertilizer</td>\n",
              "      <td>False</td>\n",
              "      <td>Twitter for Agriculture, Environment, Crops, S...</td>\n",
              "      <td>2</td>\n",
              "      <td>2142</td>\n",
              "      <td>4613</td>\n",
              "      <td>33</td>\n",
              "      <td>Bangkok, Thailand</td>\n",
              "      <td>sootin claimon</td>\n",
              "      <td>SoilFertilizer</td>\n",
              "      <td>414636</td>\n",
              "      <td>NaN</td>\n",
              "      <td>http://soclaimon.wordpress.com/</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NaN</td>\n",
              "      <td>Sun Nov 01 03:52:52 +0000 2020</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>1322748414463979520</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>en</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
              "      <td>COVID-19 = Plague</td>\n",
              "      <td>https://twitter.com/TheMJ_OfHustlin/status/132...</td>\n",
              "      <td>Wed Sep 22 03:00:31 +0000 2010</td>\n",
              "      <td>TheMJ_OfHustlin</td>\n",
              "      <td>False</td>\n",
              "      <td>#ùó†ùóøùó¶ùóµùóºùóΩùó™ùó∂ùòÅùóµùó†ùó≤‚ô¶Ô∏èùó∂ùó∫ ùóºùóª ùòÅùòÑùó∂ùòÅùòÅùó≤ùóø ùóØùòÇùòÅ ùó∂ùó∫ ùóªùóºùòÅ ùóÆ ùòÅùòÑùó∂ùòÅ...</td>\n",
              "      <td>15194</td>\n",
              "      <td>2041</td>\n",
              "      <td>877</td>\n",
              "      <td>9</td>\n",
              "      <td>Dunder Mifflin</td>\n",
              "      <td>Dr. ShopWithMe‚Ñ¢Ô∏è üç≠üÜì</td>\n",
              "      <td>TheMJ_OfHustlin</td>\n",
              "      <td>89685</td>\n",
              "      <td>NaN</td>\n",
              "      <td>https://soundcloud.com/fuck12-1</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NaN</td>\n",
              "      <td>Sun Nov 01 03:52:42 +0000 2020</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>https://twitter.com/wdunlap/status/13226760205...</td>\n",
              "      <td>0</td>\n",
              "      <td>1322748372055433216</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>en</td>\n",
              "      <td>NaN</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
              "      <td>Truly truly sad End the lockdowns re-elect Tru...</td>\n",
              "      <td>https://twitter.com/TCiffo/status/132274837205...</td>\n",
              "      <td>Sat Mar 19 12:44:11 +0000 2016</td>\n",
              "      <td>TCiffo</td>\n",
              "      <td>True</td>\n",
              "      <td>NaN</td>\n",
              "      <td>24629</td>\n",
              "      <td>88</td>\n",
              "      <td>239</td>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Trisha L. Ciffo</td>\n",
              "      <td>TCiffo</td>\n",
              "      <td>30268</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NaN</td>\n",
              "      <td>Sun Nov 01 03:52:34 +0000 2020</td>\n",
              "      <td>MurdererInChief</td>\n",
              "      <td>https://twitter.com/LaylaFanucci/status/132274...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>1322748339872440320</td>\n",
              "      <td>realDonaldTrump</td>\n",
              "      <td>1.322720e+18</td>\n",
              "      <td>25073877.0</td>\n",
              "      <td>en</td>\n",
              "      <td>NaN</td>\n",
              "      <td>True</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
              "      <td>@realDonaldTrump A group of Stanford Universit...</td>\n",
              "      <td>https://twitter.com/LaylaFanucci/status/132274...</td>\n",
              "      <td>Wed Nov 16 12:59:30 +0000 2011</td>\n",
              "      <td>LaylaFanucci</td>\n",
              "      <td>False</td>\n",
              "      <td>Layla Fanucci is an International Artist and a...</td>\n",
              "      <td>30547</td>\n",
              "      <td>385</td>\n",
              "      <td>551</td>\n",
              "      <td>8</td>\n",
              "      <td>St Helena, California</td>\n",
              "      <td>Layla Fanucci</td>\n",
              "      <td>LaylaFanucci</td>\n",
              "      <td>95534</td>\n",
              "      <td>NaN</td>\n",
              "      <td>http://laylafanucci.com</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>NaN</td>\n",
              "      <td>Sun Nov 01 03:52:26 +0000 2020</td>\n",
              "      <td>COVID19</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>1322748305898573825</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>en</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
              "      <td>A month or so ago around the 26k COV19 cases l...</td>\n",
              "      <td>https://twitter.com/ExcelYourself/status/13227...</td>\n",
              "      <td>Mon Oct 11 14:03:38 +0000 2010</td>\n",
              "      <td>ExcelYourself</td>\n",
              "      <td>False</td>\n",
              "      <td>Author, Excel Expert, CPA. I write the Excel Y...</td>\n",
              "      <td>7045</td>\n",
              "      <td>1010</td>\n",
              "      <td>783</td>\n",
              "      <td>107</td>\n",
              "      <td>Western Australia</td>\n",
              "      <td>Neale Blackwood</td>\n",
              "      <td>ExcelYourself</td>\n",
              "      <td>34576</td>\n",
              "      <td>NaN</td>\n",
              "      <td>http://a4accounting.com.au</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  coordinates  ... user_verified\n",
              "0         NaN  ...         False\n",
              "1         NaN  ...         False\n",
              "2         NaN  ...         False\n",
              "3         NaN  ...         False\n",
              "4         NaN  ...         False\n",
              "\n",
              "[5 rows x 34 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjZVe6kdXuYr"
      },
      "source": [
        "tweets = df.text.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uj4s_VuwSJBx"
      },
      "source": [
        "processed_tweets = list(map(text_prepare, tweets))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWVM8DHNTWLi"
      },
      "source": [
        "assert len(tweets) == len(df) == len(processed_tweets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRQP3p9USP-i",
        "outputId": "4f66392c-db9a-4c2f-f977-7617ea47b568"
      },
      "source": [
        "processed_tweets[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The coronavirus keeps most London theaters dark, while performers stock grocery shelves #SootinClaimon.Com https://t.co/QIClzfE6Zw',\n",
              " 'COVID-19 = Plague',\n",
              " 'Truly truly sad End the lockdowns re-elect Trumpüá∫üá∏the choice is clear! https://t.co/uvz04Yg7A5',\n",
              " '@realDonaldTrump A group of Stanford University economists estimates that there have been at least 30,000 coronavirus infections and 700 deaths as a result of 18 campaign rallies President Trump held between June and September. #MurdererInChief https://t.co/JBnmASpmJH',\n",
              " 'A month or so ago around the 26k COV19 cases level Australia + Austria were very close on both total cases + daily numbers. Australia has just had a zero day with 27k total cases + Austria has 104k total cases and 5k per day. Things can change rapidly with #COVID19']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_ESXxBcOlbE"
      },
      "source": [
        "infersent = infersent.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MV5dO6xIZQ7s",
        "outputId": "b981f952-c5ed-4b8b-92ff-c38dfec54d26"
      },
      "source": [
        "infersent.is_cuda()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOd8tZwKS8nv"
      },
      "source": [
        "all_tweets_emb = infersent.encode(processed_tweets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRxmtKIGTBRV",
        "outputId": "451743c7-099b-4a2e-f678-49ce2c210729"
      },
      "source": [
        "all_tweets_emb.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1177, 4096)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8LC_btcSQsL",
        "outputId": "44b878d3-52c5-4a96-d37f-66201e81b6e0"
      },
      "source": [
        "all_scores = np.zeros((len(tweets), len(tweets)))\n",
        "for i in range(len(processed_tweets)):\n",
        "  candidate_emb = infersent.encode([processed_tweets[i]])\n",
        "  all_scores[i] = cosine(np.repeat(candidate_emb, len(processed_tweets), axis=0), all_tweets_emb)\n",
        "all_scores[np.isnan(all_scores)] = -np.inf"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: RuntimeWarning: invalid value encountered in log\n",
            "  from ipykernel import kernelapp as app\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ko7_Z-P1TQ4n",
        "outputId": "55f8d4e3-1792-4e85-8d05-3100626e5e7e"
      },
      "source": [
        "all_scores"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.99999964,        -inf, -0.12126875, ..., -0.83017373,\n",
              "         0.17898244,  0.40660894],\n",
              "       [       -inf,  1.00000012, -2.23927355, ..., -0.15725315,\n",
              "        -0.871364  ,        -inf],\n",
              "       [-0.12126875, -2.23927402,  1.        , ..., -0.16158199,\n",
              "        -0.29706645,  0.08433688],\n",
              "       ...,\n",
              "       [-0.83017361, -0.15725315, -0.16158211, ...,  1.00000012,\n",
              "        -0.17319942, -0.10724366],\n",
              "       [ 0.17898244, -0.87136412, -0.29706645, ..., -0.1731993 ,\n",
              "         0.9999997 ,  0.1917876 ],\n",
              "       [ 0.40660882,        -inf,  0.08433682, ..., -0.10724354,\n",
              "         0.1917876 ,  0.99999988]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcRCznMoYpRH"
      },
      "source": [
        "with open('adjacency_matrix.npy', 'wb') as f:\n",
        "  np.save(f, all_scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7__zKF5bQS7T",
        "outputId": "2bcf65c3-5c9e-4390-a517-4e48ec644697"
      },
      "source": [
        "all_scores"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.99999964,        -inf, -0.12126875, ..., -0.83017373,\n",
              "         0.17898244,  0.40660894],\n",
              "       [       -inf,  1.00000012, -2.23927355, ..., -0.15725315,\n",
              "        -0.871364  ,        -inf],\n",
              "       [-0.12126875, -2.23927402,  1.        , ..., -0.16158199,\n",
              "        -0.29706645,  0.08433688],\n",
              "       ...,\n",
              "       [-0.83017361, -0.15725315, -0.16158211, ...,  1.00000012,\n",
              "        -0.17319942, -0.10724366],\n",
              "       [ 0.17898244, -0.87136412, -0.29706645, ..., -0.1731993 ,\n",
              "         0.9999997 ,  0.1917876 ],\n",
              "       [ 0.40660882,        -inf,  0.08433682, ..., -0.10724354,\n",
              "         0.1917876 ,  0.99999988]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4WFD2I9QVRo"
      },
      "source": [
        "with open('adjacency_matrix.npy', 'rb') as f:\n",
        "  all_scores = np.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPy6hFQ5QfKJ"
      },
      "source": [
        "[tweet0: for all tweet similartiy score]\n",
        "[tweet1: for all tweet similartiy score]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycDbzx0KW52m",
        "outputId": "863c8e7b-3e7e-42f5-e31a-912daead15a4"
      },
      "source": [
        "all_scores.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1177, 1177)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urBz6sHTW8k3"
      },
      "source": [
        "sorted_row_idx = np.argsort(all_scores, axis=1)[:,all_scores.shape[1]-6::]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fJZrG6tjobY",
        "outputId": "99f60f79-77cb-4a82-9fde-43109527f8c1"
      },
      "source": [
        "sorted_row_idx"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1102,  996,  988,  155,  298,    0],\n",
              "       [ 300,  396,  715,  595, 1104,    1],\n",
              "       [  85,  215,  733, 1169,  145,    2],\n",
              "       ...,\n",
              "       [ 121,  774,  150,  170,  745, 1174],\n",
              "       [ 361,  129,  280,  901, 1001, 1175],\n",
              "       [  10,  933,  822,  855,  879, 1176]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "Ypq5cvpDjs9b",
        "outputId": "71c1a534-74fd-47f9-dc03-4e7159fe4271"
      },
      "source": [
        "tweets[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The coronavirus keeps most London theaters dark, while performers stock grocery shelves #SootinClaimon.Com https://t.co/QIClzfE6Zw'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "-m5F5jIhj70P",
        "outputId": "5ba62109-7107-4786-a10c-01fcd36ff584"
      },
      "source": [
        "tweets[298]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Coronavirus updates LIVE: Victoria, NSW record zero new cases; Australia on track for internal travel bubble by Christmas; England enters national lockdown with retail, hospitality\\xa0closed https://t.co/OTUnqcl3nR'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzO3HUxZj9lM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}