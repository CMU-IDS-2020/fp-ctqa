{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "training.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnHGW6DOG6aE"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCWQ8xAA_kzl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "20530b98-769f-4820-fb36-1ea6690964c8"
      },
      "source": [
        "import time\n",
        "import os\n",
        "import numpy as np\n",
        "import google.colab as colab\n",
        "import random\n",
        "import json\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from multiprocessing import Pool\n",
        "import shutil\n",
        "from pprint import pprint\n",
        "import pickle\n",
        "from random import randint\n",
        "import pandas as pd\n",
        "\n",
        "import re\n",
        "import inspect\n",
        "from torch import optim\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfs2-AL8JD0i"
      },
      "source": [
        "# Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xykwbqUrJJUg"
      },
      "source": [
        "def mount_google_drive():\n",
        "\t'''\n",
        "\t# Functionality\n",
        "\t\tMount google drive. Since colab does not save files, we want to make it easier to directly access files in google drive.\n",
        "\t# Arguments\n",
        "\t\tNothing\n",
        "\t# Returns\n",
        "\t\tdrive_root: the working directory mounted\n",
        "\t'''\n",
        "\tmount_directory = \"/content/gdrive\"\n",
        "\tdrive = colab.drive\n",
        "\tdrive.mount(mount_directory, force_remount=True)\n",
        "\tdrive_root = mount_directory + \"/\" + list(filter(lambda x: x[0] != '.', os.listdir(mount_directory)))[0]\n",
        "\treturn drive_root"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hq7hUZxgJK7S",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "ba519894-7838-4888-e55c-a5b168344dce"
      },
      "source": [
        "# Please Set up mounted directories here. Notice whether you want to balance dataset\n",
        "ROOT_DIR =  mount_google_drive()\n",
        "\n",
        "DATASET_PATH = ROOT_DIR + \"/toy-dataset/quora.csv\"\n",
        "\n",
        "NLI_NET_DIR = ROOT_DIR + \"/models/NliNetUtils/\"\n",
        "\n",
        "CHECKPOINT_DIR = ROOT_DIR + \"/checkpoints/e2e_SNLI/\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcSJYLrLNL9F"
      },
      "source": [
        "# Migrate utils from drive to current dir so that we don't need to upload a folder from local every time\n",
        "shutil.rmtree('utils/', ignore_errors=True)\n",
        "_ = shutil.copytree(ROOT_DIR +\"/utils/\", \"utils/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2j5FQDGmKQ54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "195eab1b-dc42-494f-b47c-9ed986dc2d34"
      },
      "source": [
        "# Load custimizable utils here\n",
        "from utils.file_utils import *\n",
        "from utils.image_utils import *\n",
        "from utils.generator_utils import *\n",
        "from utils.tqdm_utils import *\n",
        "from utils.keras_utils import *"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEXNCE090hAn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "207fd1a7-81a1-4da4-8471-dbd9ae256e25"
      },
      "source": [
        "# Load infersent model related files\n",
        "shutil.rmtree('models.py', ignore_errors=True)\n",
        "shutil.copy(NLI_NET_DIR + \"models.py\", \"models.py\")\n",
        "\n",
        "shutil.rmtree('data.py', ignore_errors=True)\n",
        "shutil.copy(NLI_NET_DIR + \"data.py\", \"data.py\")\n",
        "\n",
        "shutil.rmtree('mutils.py', ignore_errors=True)\n",
        "shutil.copy(NLI_NET_DIR + \"mutils.py\", \"mutils.py\")\n",
        "\n",
        "shutil.rmtree('fastText/', ignore_errors=True)\n",
        "shutil.copytree(ROOT_DIR + \"/toy-dataset/fastText/\", \"fastText/\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'fastText/'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDrpU37uJR3g"
      },
      "source": [
        "from data import get_nli, get_batch, build_vocab\n",
        "from mutils import get_optimizer\n",
        "from models import NLINet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "paLUu7Y5HToJ"
      },
      "source": [
        "def get_optimizer(s):\n",
        "    \"\"\"\n",
        "    Parse optimizer parameters.\n",
        "    Input should be of the form:\n",
        "        - \"sgd,lr=0.01\"\n",
        "        - \"adagrad,lr=0.1,lr_decay=0.05\"\n",
        "    \"\"\"\n",
        "    if \",\" in s:\n",
        "        method = s[:s.find(',')]\n",
        "        optim_params = {}\n",
        "        for x in s[s.find(',') + 1:].split(','):\n",
        "            split = x.split('=')\n",
        "            assert len(split) == 2\n",
        "            assert re.match(\"^[+-]?(\\d+(\\.\\d*)?|\\.\\d+)$\", split[1]) is not None\n",
        "            optim_params[split[0]] = float(split[1])\n",
        "    else:\n",
        "        method = s\n",
        "        optim_params = {}\n",
        "\n",
        "    if method == 'adadelta':\n",
        "        optim_fn = optim.Adadelta\n",
        "    elif method == 'adagrad':\n",
        "        optim_fn = optim.Adagrad\n",
        "    elif method == 'adam':\n",
        "        optim_fn = optim.Adam\n",
        "    elif method == 'adamax':\n",
        "        optim_fn = optim.Adamax\n",
        "    elif method == 'asgd':\n",
        "        optim_fn = optim.ASGD\n",
        "    elif method == 'rmsprop':\n",
        "        optim_fn = optim.RMSprop\n",
        "    elif method == 'rprop':\n",
        "        optim_fn = optim.Rprop\n",
        "    elif method == 'sgd':\n",
        "        optim_fn = optim.SGD\n",
        "        assert 'lr' in optim_params\n",
        "    else:\n",
        "        raise Exception('Unknown optimization method: \"%s\"' % method)\n",
        "\n",
        "    # check that we give good parameters to the optimizer\n",
        "    expected_args = inspect.getargspec(optim_fn.__init__)[0]\n",
        "    assert expected_args[:2] == ['self', 'params']\n",
        "    if not all(k in expected_args[2:] for k in optim_params.keys()):\n",
        "        raise Exception('Unexpected parameters: expected \"%s\", got \"%s\"' % (\n",
        "            str(expected_args[2:]), str(optim_params.keys())))\n",
        "\n",
        "    return optim_fn, optim_params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8T5Yf68GQMLE"
      },
      "source": [
        "# Look At Your Data First"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SG40LXPdN5-m"
      },
      "source": [
        "def preprocess_df(quora_df, balance=True):  \n",
        "  quora_df['question1'] = quora_df['question1'].astype(str) \n",
        "  quora_df['question2'] = quora_df['question2'].astype(str)\n",
        "  if not balance:\n",
        "    premise_sents = quora_df['question1'].tolist()\n",
        "    hypothesis_sents = quora_df['question2'].tolist()\n",
        "    labels = quora_df['is_duplicate'].tolist() \n",
        "  else:\n",
        "    diff_df = quora_df.loc[quora_df['is_duplicate'] == 0]\n",
        "    simi_df = quora_df.loc[quora_df['is_duplicate'] == 1]\n",
        "    \n",
        "    balance_size = min(diff_df.shape[0], simi_df.shape[0])  \n",
        "    updated_diff_df = diff_df.sample(n=balance_size, random_state = 517)\n",
        "    \n",
        "    # No need to shuffle here because train_test_split will take care of it\n",
        "  \n",
        "    premise_sents = simi_df['question1'].tolist()\n",
        "    premise_sents += updated_diff_df['question1'].tolist()\n",
        "    \n",
        "    hypothesis_sents = simi_df['question2'].tolist()\n",
        "    hypothesis_sents += updated_diff_df['question2'].tolist()\n",
        "    \n",
        "    labels = simi_df['is_duplicate'].tolist()\n",
        "    labels += updated_diff_df['is_duplicate'].tolist()\n",
        "  \n",
        "  return premise_sents, hypothesis_sents, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkT8PBLUOocE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "fb1d8915-7e40-4818-8f8d-334796e57101"
      },
      "source": [
        "quora_df = pd.read_csv(DATASET_PATH)\n",
        "quora_df.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>qid1</th>\n",
              "      <th>qid2</th>\n",
              "      <th>question1</th>\n",
              "      <th>question2</th>\n",
              "      <th>is_duplicate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>What is the step by step guide to invest in sh...</td>\n",
              "      <td>What is the step by step guide to invest in sh...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
              "      <td>What would happen if the Indian government sto...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>How can I increase the speed of my internet co...</td>\n",
              "      <td>How can Internet speed be increased by hacking...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
              "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>9</td>\n",
              "      <td>10</td>\n",
              "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
              "      <td>Which fish would survive in salt water?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>11</td>\n",
              "      <td>12</td>\n",
              "      <td>Astrology: I am a Capricorn Sun Cap moon and c...</td>\n",
              "      <td>I'm a triple Capricorn (Sun, Moon and ascendan...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>13</td>\n",
              "      <td>14</td>\n",
              "      <td>Should I buy tiago?</td>\n",
              "      <td>What keeps childern active and far from phone ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>15</td>\n",
              "      <td>16</td>\n",
              "      <td>How can I be a good geologist?</td>\n",
              "      <td>What should I do to be a great geologist?</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>17</td>\n",
              "      <td>18</td>\n",
              "      <td>When do you use シ instead of し?</td>\n",
              "      <td>When do you use \"&amp;\" instead of \"and\"?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>19</td>\n",
              "      <td>20</td>\n",
              "      <td>Motorola (company): Can I hack my Charter Moto...</td>\n",
              "      <td>How do I hack Motorola DCX3400 for free internet?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  qid1  ...                                          question2 is_duplicate\n",
              "0   0     1  ...  What is the step by step guide to invest in sh...            0\n",
              "1   1     3  ...  What would happen if the Indian government sto...            0\n",
              "2   2     5  ...  How can Internet speed be increased by hacking...            0\n",
              "3   3     7  ...  Find the remainder when [math]23^{24}[/math] i...            0\n",
              "4   4     9  ...            Which fish would survive in salt water?            0\n",
              "5   5    11  ...  I'm a triple Capricorn (Sun, Moon and ascendan...            1\n",
              "6   6    13  ...  What keeps childern active and far from phone ...            0\n",
              "7   7    15  ...          What should I do to be a great geologist?            1\n",
              "8   8    17  ...              When do you use \"&\" instead of \"and\"?            0\n",
              "9   9    19  ...  How do I hack Motorola DCX3400 for free internet?            0\n",
              "\n",
              "[10 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGl9c11I3yPo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "39979590-0434-46d0-e02a-d40b7cd71bc6"
      },
      "source": [
        "premise_sents, hypothesis_sents, labels = preprocess_df(quora_df, True)\n",
        "assert len(premise_sents) == len(hypothesis_sents) == len(labels)\n",
        "print(\"Total number of sentences in the dataset is\", str(len(labels)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of sentences in the dataset is 298612\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjWpyaAlz63V"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKP-euuZ0SBI"
      },
      "source": [
        "import re\n",
        "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
        "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
        "\n",
        "def text_prepare(text):\n",
        "    \"\"\"\n",
        "        text: a string\n",
        "        \n",
        "        return: modified initial string\n",
        "    \"\"\"\n",
        "    text = str(text)\n",
        "    text = text.lower() # lowercase text\n",
        "    text = re.sub(REPLACE_BY_SPACE_RE, \" \", text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
        "    text = re.sub(BAD_SYMBOLS_RE, \"\", text)      # delete symbols which are in BAD_SYMBOLS_RE from text\n",
        "    text = re.sub(' +', ' ', text)\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaK0Vfaq3f0M"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "premise_prep = list(map(text_prepare, premise_sents))\n",
        "hypothesis_prep = list(map(text_prepare, hypothesis_sents))\n",
        "\n",
        "X = [(premise_prep[i], hypothesis_prep[i]) for i in range(len(labels))]\n",
        "\n",
        "y = labels\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=46)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=46)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIy_7eS43f2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b5bbe377-117e-4779-bbed-e5b891097ec7"
      },
      "source": [
        "# Check if test set are balanced\n",
        "counter = 0\n",
        "for element in y_test:\n",
        "  if element == 0:\n",
        "    counter +=1 \n",
        "\n",
        "print(counter)\n",
        "print(len(y_test) - counter)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14944\n",
            "14918\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6monVwe3f4i"
      },
      "source": [
        "def get_data(X, y):\n",
        "  left = [s1 for s1, s2 in X]\n",
        "  right = [s2 for s1, s2 in X]\n",
        "\n",
        "  LABELS = {'different': 0, 'similar': 1}\n",
        "#   Y = np_utils.to_categorical(np.array(y), len(LABELS))\n",
        " \n",
        "  return [left, right, np.array(y)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9u0p6Pp31s1"
      },
      "source": [
        "training = get_data(X_train, y_train)\n",
        "validation = get_data(X_val, y_val)\n",
        "testing = get_data(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4r4x70N3474i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "43969678-85a6-4600-f237-e244f026bea9"
      },
      "source": [
        "testing[1][:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['would banning notes of denominations 500 and 1000 help to curb the black money in system',\n",
              " 'which country would be the best for setting up a natural cancer clinic bearing in mind the setup cost governmental regulations and the degree of red tape etc',\n",
              " 'is the damage to prefrontal cortex reversible',\n",
              " 'how much percentile should i aim for to get into an iim in cat 2016',\n",
              " 'what are the best ways to build a vacuole model']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UylW2bRC4bMQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "70f05928-d9c6-487b-a068-79625d084d24"
      },
      "source": [
        "word_vec = build_vocab(training[0] + training[1] +\n",
        "                       validation[0] + validation[1] +\n",
        "                       testing[0] + testing[1], ROOT_DIR + \"/toy-dataset/glove.840B.300d.txt\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 58735(/79667) words with glove vectors\n",
            "Vocab size : 58735\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_1XmfpC39v2"
      },
      "source": [
        "def preprocess(sentences):\n",
        "  return np.array([  ['<s>'] +\n",
        "              [word for word in sent.split() if word in word_vec] +\n",
        "              ['</s>'] \n",
        "              for sent in sentences])\n",
        "\n",
        "for index in [0, 1]:\n",
        "  training[index] = preprocess(training[index])\n",
        "  validation[index] = preprocess(validation[index])\n",
        "  testing[index] = preprocess(testing[index])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHJGwIPM39yJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "1d614b95-4033-409f-ed23-54cfd8fa0f78"
      },
      "source": [
        "training[0][0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<s>',\n",
              " 'harvard',\n",
              " 'college',\n",
              " 'courses',\n",
              " 'what',\n",
              " 'is',\n",
              " 'general',\n",
              " 'shopping',\n",
              " 'advice',\n",
              " 'for',\n",
              " 'german',\n",
              " 'classes',\n",
              " '</s>']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b01kkCIGSlzG"
      },
      "source": [
        "config_nli_model = {\n",
        "    'n_words'        :  len(word_vec)         , # Number of distinct words in the wordvec\n",
        "    'word_emb_dim'   :  300                   , # Dimension of word embeddings\n",
        "    'dpout_model'    :  0.                    , # Dropout\n",
        "    'enc_lstm_dim'   :  2048                  ,\n",
        "    'dpout_fc'       :  0.5                   ,\n",
        "    'fc_dim'         :  512                   ,\n",
        "    'bsize'          :  64                    ,\n",
        "    'n_classes'      :  2                     ,\n",
        "    'pool_type'      :  'max'                 ,\n",
        "    'nonlinear_fc'   :  0                     ,\n",
        "    'encoder_type'   :  'InferSent'           , # see list of encoders\n",
        "    'use_cuda'       :  True                  ,\n",
        "    'optimizer'      :  \"adam\"         ,\n",
        "    'decay'          :  0.99                  ,\n",
        "    'max_norm'       :  5.                    ,\n",
        "    'minlr'          :  1e-5                  ,\n",
        "    'outputdir'      :  CHECKPOINT_DIR        ,\n",
        "    'outputmodelname':  'e2dmodel.pickle'     ,\n",
        "    'lrshrink'       :  5                     ,\n",
        "    'n_epochs'       :  30\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LQOMeUMBoop"
      },
      "source": [
        "# Model Architecture\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdUiIwgsirhY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "420bc2cb-49c4-4e32-db55-a1f5e39e2ca1"
      },
      "source": [
        "nli_net = NLINet(config_nli_model)\n",
        "print(nli_net)\n",
        "\n",
        "\n",
        "# loss\n",
        "weight = torch.FloatTensor(config_nli_model['n_classes']).fill_(1)\n",
        "loss_fn = nn.CrossEntropyLoss(weight=weight)\n",
        "loss_fn.size_average = False\n",
        "\n",
        "# optimizer\n",
        "optim_fn, optim_parameters = get_optimizer(config_nli_model['optimizer'])\n",
        "optimizer = optim_fn(nli_net.parameters(), **optim_parameters)\n",
        "\n",
        "# cuda by default\n",
        "nli_net.cuda()\n",
        "loss_fn.cuda()\n",
        "\n",
        "\n",
        "val_acc_best = -1e10\n",
        "adam_stop = False\n",
        "stop_training = False\n",
        "lr = optim_parameters['lr'] if 'sgd' in config_nli_model['optimizer'] else None"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NLINet(\n",
            "  (encoder): InferSent(\n",
            "    (enc_lstm): LSTM(300, 2048, bidirectional=True)\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=16384, out_features=2, bias=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rS630XCbfP2B"
      },
      "source": [
        "# some name changes...\n",
        "train = training\n",
        "valid = validation\n",
        "test = testing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKphHVvSpiI1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e9628f64-21b0-42c0-b9c5-d59c6bd82cbb"
      },
      "source": [
        " train[2][1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYcCOJy_KLXv"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgUSo1CmYaf4"
      },
      "source": [
        "def trainepoch(epoch):\n",
        "    print('\\nTRAINING : Epoch ' + str(epoch))\n",
        "    nli_net.train()\n",
        "    all_costs = []\n",
        "    logs = []\n",
        "    words_count = 0\n",
        "\n",
        "    last_time = time.time()\n",
        "    correct = 0.\n",
        "    # shuffle the data\n",
        "    permutation = np.random.permutation(len(train[0]))\n",
        "\n",
        "    s1 = train[0][permutation]\n",
        "    s2 = train[1][permutation]\n",
        "    target = train[2][permutation]\n",
        "\n",
        "\n",
        "    optimizer.param_groups[0]['lr'] = optimizer.param_groups[0]['lr'] * config_nli_model['decay'] if epoch>1\\\n",
        "        and 'sgd' in config_nli_model['optimizer'] else optimizer.param_groups[0]['lr']\n",
        "    print('Learning rate : {0}'.format(optimizer.param_groups[0]['lr']))\n",
        "\n",
        "    for stidx in range(0, len(s1), config_nli_model['bsize']):\n",
        "        # prepare batch\n",
        "        s1_batch, s1_len = get_batch(s1[stidx:stidx + config_nli_model['bsize']],\n",
        "                                     word_vec, config_nli_model['word_emb_dim'])\n",
        "        s2_batch, s2_len = get_batch(s2[stidx:stidx + config_nli_model['bsize']],\n",
        "                                     word_vec, config_nli_model['word_emb_dim'])\n",
        "        s1_batch, s2_batch = Variable(s1_batch.cuda()), Variable(s2_batch.cuda())\n",
        "        tgt_batch = Variable(torch.LongTensor(target[stidx:stidx + config_nli_model['bsize']])).cuda()\n",
        "        k = s1_batch.size(1)  # actual batch size\n",
        "\n",
        "        # model forward\n",
        "        output = nli_net((s1_batch, s1_len), (s2_batch, s2_len))\n",
        "\n",
        "        pred = output.data.max(1)[1]\n",
        "        correct += pred.long().eq(tgt_batch.data.long()).cpu().sum()\n",
        "        assert len(pred) == len(s1[stidx:stidx + config_nli_model['bsize']])\n",
        "\n",
        "        # loss\n",
        "        loss = loss_fn(output, tgt_batch)\n",
        "        all_costs.append(loss.data.item())\n",
        "        words_count += (s1_batch.nelement() + s2_batch.nelement()) / config_nli_model['word_emb_dim']\n",
        "\n",
        "        # backward\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # gradient clipping (off by default)\n",
        "        shrink_factor = 1\n",
        "        total_norm = 0\n",
        "\n",
        "        for p in nli_net.parameters():\n",
        "            if p.requires_grad:\n",
        "                p.grad.data.div_(k)  # divide by the actual batch size\n",
        "                total_norm += p.grad.data.norm() ** 2\n",
        "        total_norm = np.sqrt(total_norm.cpu())\n",
        "\n",
        "        if total_norm > config_nli_model['max_norm']:\n",
        "            shrink_factor = config_nli_model['max_norm'] / total_norm\n",
        "        current_lr = optimizer.param_groups[0]['lr'] # current lr (no external \"lr\", for adam)\n",
        "        optimizer.param_groups[0]['lr'] = current_lr * shrink_factor # just for update\n",
        "\n",
        "        # optimizer step\n",
        "        optimizer.step()\n",
        "        optimizer.param_groups[0]['lr'] = current_lr\n",
        "\n",
        "        if len(all_costs) == 1000:\n",
        "            logs.append('{0} ; loss {1} ; sentence/s {2} ; words/s {3} ; accuracy train : {4}'.format(\n",
        "                            stidx, \n",
        "                            round(np.mean(all_costs), 5),\n",
        "                            int(len(all_costs) * config_nli_model['bsize'] / (time.time() - last_time)),\n",
        "                            int(words_count * 1.0 / (time.time() - last_time)),\n",
        "                            round(100.*correct.data.item()/(stidx+k), 2)))\n",
        "            print(logs[-1])\n",
        "            last_time = time.time()\n",
        "            words_count = 0\n",
        "            all_costs = []\n",
        "    train_acc = round(100 * correct.data.item()/len(s1), 2)\n",
        "    print('results : epoch {0} ; mean accuracy train : {1}'\n",
        "          .format(epoch, train_acc))\n",
        "    return train_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjul12UHYpdE"
      },
      "source": [
        "def evaluate(epoch, eval_type='valid', final_eval=False):\n",
        "    nli_net.eval()\n",
        "    correct = 0.\n",
        "    global val_acc_best, lr, stop_training, adam_stop\n",
        "\n",
        "    if eval_type == 'valid':\n",
        "        print('\\nVALIDATION : Epoch {0}'.format(epoch))\n",
        "\n",
        "    s1 = valid[0] if eval_type == 'valid' else test[0]\n",
        "    s2 = valid[1] if eval_type == 'valid' else test[1]\n",
        "    target = valid[2] if eval_type == 'valid' else test[2]\n",
        "\n",
        "    for i in range(0, len(s1), config_nli_model['bsize']):\n",
        "        # prepare batch\n",
        "        s1_batch, s1_len = get_batch(s1[i:i + config_nli_model['bsize']], word_vec, config_nli_model['word_emb_dim'])\n",
        "        s2_batch, s2_len = get_batch(s2[i:i + config_nli_model['bsize']], word_vec, config_nli_model['word_emb_dim'])\n",
        "        s1_batch, s2_batch = Variable(s1_batch.cuda()), Variable(s2_batch.cuda())\n",
        "        tgt_batch = Variable(torch.LongTensor(target[i:i + config_nli_model['bsize']])).cuda()\n",
        "\n",
        "        # model forward\n",
        "        output = nli_net((s1_batch, s1_len), (s2_batch, s2_len))\n",
        "\n",
        "        pred = output.data.max(1)[1]\n",
        "        correct += pred.long().eq(tgt_batch.data.long()).cpu().sum()\n",
        "\n",
        "    # save model\n",
        "    eval_acc = round(100 * correct.data.item()/ len(s1), 3)\n",
        "    if final_eval:\n",
        "        print('finalgrep : accuracy {0} : {1}'.format(eval_type, eval_acc))\n",
        "    else:\n",
        "        print('togrep : results : epoch {0} ; mean accuracy {1} :\\\n",
        "              {2}'.format(epoch, eval_type, eval_acc))\n",
        "\n",
        "    if eval_type == 'valid' and epoch <= config_nli_model['n_epochs']:\n",
        "        if eval_acc > val_acc_best:\n",
        "            print('saving model at epoch {0}'.format(epoch))\n",
        "            if not os.path.exists(config_nli_model['outputdir']):\n",
        "                os.makedirs(config_nli_model['outputdir'])\n",
        "            torch.save(nli_net.state_dict(), os.path.join(config_nli_model['outputdir'],\n",
        "                       config_nli_model['outputmodelname']))\n",
        "            val_acc_best = eval_acc\n",
        "        else:\n",
        "            if 'sgd' in config_nli_model['optimizer']:\n",
        "                optimizer.param_groups[0]['lr'] = optimizer.param_groups[0]['lr'] / config_nli_model['lrshrink']\n",
        "                print('Shrinking lr by : {0}. New lr = {1}'\n",
        "                      .format(config_nli_model['lrshrink'],\n",
        "                              optimizer.param_groups[0]['lr']))\n",
        "                if optimizer.param_groups[0]['lr'] < config_nli_model['minlr']:\n",
        "                    stop_training = True\n",
        "            if 'adam' in config_nli_model['optimizer']:\n",
        "                # early stopping (at 2nd decrease in accuracy)\n",
        "                stop_training = adam_stop\n",
        "                adam_stop = True\n",
        "    return eval_acc\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QmGW-vSYpfX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fbb1a642-df6b-4066-87e3-1d0866ca5d00"
      },
      "source": [
        "\"\"\"\n",
        "Train model on Natural Language Inference task\n",
        "\"\"\"\n",
        "epoch = 1\n",
        "\n",
        "train_history = []\n",
        "val_history = []\n",
        "\n",
        "while not stop_training and epoch <= config_nli_model['n_epochs']:\n",
        "    train_acc = trainepoch(epoch)\n",
        "    eval_acc = evaluate(epoch, 'valid')\n",
        "    epoch += 1\n",
        "    train_history.append(train_acc)\n",
        "    val_history.append(eval_acc)\n",
        "\n",
        "# Run best model on test set.\n",
        "nli_net.load_state_dict(torch.load(os.path.join(config_nli_model['outputdir'], config_nli_model['outputmodelname'])))\n",
        "\n",
        "print('\\nTEST : Epoch {0}'.format(epoch))\n",
        "evaluate(1e6, 'valid', True)\n",
        "evaluate(0, 'test', True)\n",
        "\n",
        "# Save encoder instead of full model\n",
        "# torch.save(nli_net.encoder.state_dict(), os.path.join(config_nli_model['outputdir'], config_nli_model['outputmodelname'] + '.encoder.pkl'))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "TRAINING : Epoch 1\n",
            "Learning rate : 0.001\n",
            "63936 ; loss 0.53297 ; sentence/s 115 ; words/s 7921 ; accuracy train : 76.74\n",
            "127936 ; loss 0.50052 ; sentence/s 115 ; words/s 7821 ; accuracy train : 78.53\n",
            "191936 ; loss 0.49187 ; sentence/s 115 ; words/s 7825 ; accuracy train : 79.43\n",
            "results : epoch 1 ; mean accuracy train : 79.86\n",
            "\n",
            "VALIDATION : Epoch 1\n",
            "togrep : results : epoch 1 ; mean accuracy valid :              81.95\n",
            "saving model at epoch 1\n",
            "\n",
            "TRAINING : Epoch 2\n",
            "Learning rate : 0.001\n",
            "63936 ; loss 0.46337 ; sentence/s 115 ; words/s 7808 ; accuracy train : 84.38\n",
            "127936 ; loss 0.46496 ; sentence/s 116 ; words/s 7897 ; accuracy train : 84.28\n",
            "191936 ; loss 0.46118 ; sentence/s 115 ; words/s 7726 ; accuracy train : 84.38\n",
            "results : epoch 2 ; mean accuracy train : 84.4\n",
            "\n",
            "VALIDATION : Epoch 2\n",
            "togrep : results : epoch 2 ; mean accuracy valid :              83.139\n",
            "saving model at epoch 2\n",
            "\n",
            "TRAINING : Epoch 3\n",
            "Learning rate : 0.001\n",
            "63936 ; loss 0.43428 ; sentence/s 118 ; words/s 7904 ; accuracy train : 87.48\n",
            "127936 ; loss 0.43703 ; sentence/s 115 ; words/s 7815 ; accuracy train : 87.36\n",
            "191936 ; loss 0.43861 ; sentence/s 114 ; words/s 7822 ; accuracy train : 87.25\n",
            "results : epoch 3 ; mean accuracy train : 87.19\n",
            "\n",
            "VALIDATION : Epoch 3\n",
            "togrep : results : epoch 3 ; mean accuracy valid :              84.07\n",
            "saving model at epoch 3\n",
            "\n",
            "TRAINING : Epoch 4\n",
            "Learning rate : 0.001\n",
            "63936 ; loss 0.41395 ; sentence/s 115 ; words/s 7758 ; accuracy train : 89.76\n",
            "127936 ; loss 0.41875 ; sentence/s 114 ; words/s 7786 ; accuracy train : 89.44\n",
            "191936 ; loss 0.41878 ; sentence/s 114 ; words/s 7845 ; accuracy train : 89.32\n",
            "results : epoch 4 ; mean accuracy train : 89.25\n",
            "\n",
            "VALIDATION : Epoch 4\n",
            "togrep : results : epoch 4 ; mean accuracy valid :              84.739\n",
            "saving model at epoch 4\n",
            "\n",
            "TRAINING : Epoch 5\n",
            "Learning rate : 0.001\n",
            "63936 ; loss 0.39792 ; sentence/s 114 ; words/s 7781 ; accuracy train : 91.42\n",
            "127936 ; loss 0.40243 ; sentence/s 115 ; words/s 7806 ; accuracy train : 91.16\n",
            "191936 ; loss 0.40547 ; sentence/s 115 ; words/s 7884 ; accuracy train : 90.96\n",
            "results : epoch 5 ; mean accuracy train : 90.86\n",
            "\n",
            "VALIDATION : Epoch 5\n",
            "togrep : results : epoch 5 ; mean accuracy valid :              85.235\n",
            "saving model at epoch 5\n",
            "\n",
            "TRAINING : Epoch 6\n",
            "Learning rate : 0.001\n",
            "63936 ; loss 0.38911 ; sentence/s 117 ; words/s 7908 ; accuracy train : 92.32\n",
            "127936 ; loss 0.39381 ; sentence/s 117 ; words/s 8011 ; accuracy train : 92.06\n",
            "191936 ; loss 0.39366 ; sentence/s 117 ; words/s 7831 ; accuracy train : 91.97\n",
            "results : epoch 6 ; mean accuracy train : 91.82\n",
            "\n",
            "VALIDATION : Epoch 6\n",
            "togrep : results : epoch 6 ; mean accuracy valid :              84.669\n",
            "\n",
            "TRAINING : Epoch 7\n",
            "Learning rate : 0.001\n",
            "63936 ; loss 0.38294 ; sentence/s 116 ; words/s 7852 ; accuracy train : 92.92\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-fa0b98946a3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstop_training\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mconfig_nli_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'n_epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainepoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0meval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'valid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mepoch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-21b54b421403>\u001b[0m in \u001b[0;36mtrainepoch\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     53\u001b[0m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# divide by the actual batch size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0mtotal_norm\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_norm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtotal_norm\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mconfig_nli_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'max_norm'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtYB0tHygC9e"
      },
      "source": [
        "# summarize history for accuracy\n",
        "plt.plot(train_history)\n",
        "plt.plot(val_history)\n",
        "plt.title('Quora e2e accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2DBdIUYmZGN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhkVOTLamZIw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZQCvrxAmZLK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2doiT7LmZEC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HE3z6Y0xYpho"
      },
      "source": [
        "def inference(s1, s2, in_set=False):\n",
        "  # s1: LIST of premise\n",
        "  # s2: LIST of hypothesis\n",
        "  \n",
        "  if not in_set:\n",
        "    s1 = preprocess(s1)\n",
        "    s2 = preprocess(s2)\n",
        "    \n",
        "  s1_batch, s1_len = get_batch(s1[0:1], word_vec, config_nli_model['word_emb_dim'])\n",
        "  s2_batch, s2_len = get_batch(s2[0:1], word_vec, config_nli_model['word_emb_dim'])\n",
        "  s1_batch, s2_batch = Variable(s1_batch.cuda()), Variable(s2_batch.cuda())\n",
        "\n",
        "  # model forward\n",
        "  output = nli_net((s1_batch, s1_len), (s2_batch, s2_len))\n",
        "  return output.data.max(1)[1].item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTuhnNLuYpkG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "c2efabb2-38fd-4bd7-fa8c-4485fc1a144f"
      },
      "source": [
        "index = 32\n",
        "\n",
        "sentence1 = train[0][index: index+2]\n",
        "sentence2 = train[1][index: index+2]\n",
        "label = train[2][index: index+2][0]\n",
        "\n",
        "\n",
        "print(sentence1[0])\n",
        "print(sentence2[0])\n",
        "print(\"Prediction is\", inference(sentence1, sentence2, in_set=True))\n",
        "print(\"Label is\", label)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['<s>', 'what', 'should', 'a', 'developer', 'do', 'to', 'become', 'a', 'top', 'developer', 'on', 'google', 'play', '</s>']\n",
            "['<s>', 'who', 'qualifies', 'to', 'be', 'a', 'top', 'developer', 'on', 'google', 'play', 'store', '</s>']\n",
            "Prediction is 0\n",
            "Label is 1\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}