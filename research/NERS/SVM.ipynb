{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import scipy.sparse as sp\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_distances, euclidean_distances\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>status_id</th>\n",
       "      <th>annotation1</th>\n",
       "      <th>annotation2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>News you can use, from a public health officia...</td>\n",
       "      <td>1236988271625936896</td>\n",
       "      <td>1236988271625936896</td>\n",
       "      <td>calling out or correction</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#Twimbos are you aware of the fake COVID-19 he...</td>\n",
       "      <td>1236985619047419904</td>\n",
       "      <td>1236985619047419904</td>\n",
       "      <td>calling out or correction</td>\n",
       "      <td>calling out or correction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No, lime juice won't immunize you: Some people...</td>\n",
       "      <td>1222615296344608768</td>\n",
       "      <td>1222615296344608768</td>\n",
       "      <td>calling out or correction</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Coronavirus Where to next? https://t.co/Xssp4...</td>\n",
       "      <td>1236981003224113152</td>\n",
       "      <td>1236981003224113152</td>\n",
       "      <td>news</td>\n",
       "      <td>ambiguous or hard to classify</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Does cocaine help with coronavirus? WHO has th...</td>\n",
       "      <td>1236980398598557696</td>\n",
       "      <td>1236980398598557696</td>\n",
       "      <td>calling out or correction</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text                   id  \\\n",
       "0  News you can use, from a public health officia...  1236988271625936896   \n",
       "1  #Twimbos are you aware of the fake COVID-19 he...  1236985619047419904   \n",
       "2  No, lime juice won't immunize you: Some people...  1222615296344608768   \n",
       "3  #Coronavirus Where to next? https://t.co/Xssp4...  1236981003224113152   \n",
       "4  Does cocaine help with coronavirus? WHO has th...  1236980398598557696   \n",
       "\n",
       "             status_id                annotation1  \\\n",
       "0  1236988271625936896  calling out or correction   \n",
       "1  1236985619047419904  calling out or correction   \n",
       "2  1222615296344608768  calling out or correction   \n",
       "3  1236981003224113152                       news   \n",
       "4  1236980398598557696  calling out or correction   \n",
       "\n",
       "                     annotation2  \n",
       "0                            NaN  \n",
       "1      calling out or correction  \n",
       "2                            NaN  \n",
       "3  ambiguous or hard to classify  \n",
       "4                            NaN  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_tweets = pd.read_csv(\"dframe.csv\")\n",
    "#df_ids = pd.read_csv(\"CMU_MisCov19_dataset.csv\")\n",
    "#df = pd.merge(df_tweets, df_ids, left_on='id', right_on='status_id')\n",
    "#df = df[['text','id','status_id','annotation1','annotation2']]\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../tweets.csv\")['text']\n",
    "df.head()\n",
    "origin = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "def lemmaSentence(sentence):\n",
    "    token_words=word_tokenize(sentence)\n",
    "    token_words\n",
    "    stem_sentence=[]\n",
    "    for word in token_words:\n",
    "        stem_sentence.append(wnl.lemmatize(word))\n",
    "        stem_sentence.append(\" \")\n",
    "    return \"\".join(stem_sentence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r\"[A-Z][a-z]+|\\d+|[A-Z]+(?![a-z])\")\n",
    "\n",
    "def split_hashtag(tag):\n",
    "    return \" \".join(pattern.findall(tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Real Donald Trump'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_hashtag(\"@RealDonaldTrump\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = df.reset_index(drop=True)\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\'\\|,;.!?=+-]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text)\n",
    "    text = ' '.join(split_hashtag(word) if word[0] is '@' or word[0] is '#' else word for word in text.split())\n",
    "    ext = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE) #remove url\n",
    "    text = lemmaSentence(text)\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwords from text\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    the coronavirus keep london theater dark perfo...\n",
       "1                                      covid 19 plague\n",
       "2    truly truly sad end lockdown elect trumpðŸ‡ºðŸ‡¸the ...\n",
       "3    donald trump a group stanford university econo...\n",
       "4    a month ago around 26k cov19 case level austra...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.apply(clean_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_vectorizer(reviews_train, reviews_test = None):\n",
    "    tf_vectorizer = TfidfVectorizer(analyzer = str.split, tokenizer = str.split, preprocessor = lambda x : x)\n",
    "    tf_train = tf_vectorizer.fit_transform(reviews_train)\n",
    "    features = list(tf_vectorizer.vocabulary_.keys())\n",
    "    features.sort()\n",
    "    \n",
    "    tf_test = None\n",
    "    if reviews_test is not None:\n",
    "        tf_test = tf_vectorizer.transform(reviews_test)\n",
    "        \n",
    "    return (tf_train, tf_test, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['anybody', 'oga', 'vaccination'],\n",
       " ['protesting', 'vahappened', 'antiva'],\n",
       " ['indeed', 'moking', '4hwcu'],\n",
       " ['hurch', 'affecting', '75cln'],\n",
       " ['stock', 'magic', 'uck'],\n",
       " ['storm', 'nited', 'riot'],\n",
       " ['seemingly', 'league', 'lobal'],\n",
       " ['coronavirus', 'http', 'tco'],\n",
       " ['wine', 'bleachhttps', 'book'],\n",
       " ['limit', 'google', 'stupidity'],\n",
       " ['il', 'ystem', 'mmune'],\n",
       " ['grain', 'ministry', 'black'],\n",
       " ['2', 'http', 'tco'],\n",
       " ['infect', 'ioeapon', 'working'],\n",
       " ['plus', 'ovt', 'shouldnt'],\n",
       " ['discussion', 'ocaine', 'ready']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf,test, features = tfidf_vectorizer(df)\n",
    "n_topics = len(np.unique(df['annotation1']))\n",
    "n_top_words = 3\n",
    "lda = LatentDirichletAllocation(n_components=n_topics, learning_method='online', random_state=0)\n",
    "lda.fit(tf)\n",
    "top_words = []\n",
    "features = np.array(features)\n",
    "for topic in range(n_topics):\n",
    "    sorted_indices = np.argsort(lda.components_[topic])\n",
    "    top_words.append(list(features[sorted_indices[-n_top_words:]]))\n",
    "    \n",
    "top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python36\\lib\\site-packages\\sklearn\\model_selection\\_split.py:672: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score 0.8695537364668603\n",
      "Test Score 0.5581947743467933\n",
      "Test Score 0.8711381040401374\n",
      "Test Score 0.5391923990498813\n",
      "Test Score 0.8753630842355427\n",
      "Test Score 0.5581947743467933\n",
      "Test Score 0.8714021653023502\n",
      "Test Score 0.5463182897862233\n",
      "Test Score 0.8698177977290732\n",
      "Test Score 0.5653206650831354\n",
      "Test Score 0.8745709004489042\n",
      "Test Score 0.5558194774346793\n",
      "Test Score 0.8721943490889886\n",
      "Test Score 0.5249406175771971\n",
      "Test Score 0.8782677581198838\n",
      "Test Score 0.5771971496437055\n",
      "Test Score 0.8685322069693769\n",
      "Test Score 0.5547619047619048\n",
      "Test Score 0.8748680042238648\n",
      "Test Score 0.5595238095238095\n"
     ]
    }
   ],
   "source": [
    "y = df['annotation1']\n",
    "X = df['text']\n",
    "\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True,random_state=0)\n",
    "\n",
    "\n",
    "model = SGDClassifier(loss='hinge', penalty='l2',alpha=1e-2, random_state=42, max_iter=15, tol=None)\n",
    "for train_indices, test_indices in skf.split(X, y):\n",
    "    X_train, X_test, features = tfidf_vectorizer(X[train_indices], X[test_indices])\n",
    "\n",
    "    y_train = y[train_indices]\n",
    "    y_test = y[test_indices]\n",
    "\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    print(\"Test Score {}\".format(model.score(X_train,y_train)))\n",
    "    print(\"Test Score {}\".format(model.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_lg'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-84-a84c1b6e782e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0men_core_web_sm\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'en_core_web_lg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0men_core_web_lg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0men_core_web_sm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python36\\lib\\site-packages\\spacy\\__init__.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(name, **overrides)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDeprecationWarning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python36\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(name, **overrides)\u001b[0m\n\u001b[0;32m    173\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"exists\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Path or Path-like to model data\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_lg'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "import en_core_web_lg\n",
    "nlp = en_core_web_sm.load()\n",
    "nlp_big = en_core_web_lg.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "donald trump group stanford university economist estimate least 30 000 coronavirus infection 700 death result 18 campaign rally president trump held june september murderer chief http : co jbnmaspmjh\n"
     ]
    }
   ],
   "source": [
    "X= df\n",
    "tags = nlp(X[3])\n",
    "print(X[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('donald', 'B', 'ORG'), ('trump', 'I', 'ORG'), ('group', 'I', 'ORG'), ('stanford', 'B', 'ORG'), ('university', 'I', 'ORG'), ('economist', 'O', ''), ('estimate', 'O', ''), ('least', 'O', ''), ('30', 'B', 'CARDINAL'), ('000', 'O', ''), ('coronavirus', 'O', ''), ('infection', 'O', ''), ('700', 'B', 'CARDINAL'), ('death', 'O', ''), ('result', 'O', ''), ('18', 'B', 'CARDINAL'), ('campaign', 'O', ''), ('rally', 'O', ''), ('president', 'O', ''), ('trump', 'O', ''), ('held', 'O', ''), ('june', 'B', 'DATE'), ('september', 'I', 'DATE'), ('murderer', 'O', ''), ('chief', 'O', ''), ('http', 'O', ''), (':', 'O', ''), ('co', 'B', 'PERSON'), ('jbnmaspmjh', 'I', 'PERSON')]\n",
      "<class 'spacy.tokens.token.Token'>\n"
     ]
    }
   ],
   "source": [
    "print([(X.text, X.ent_iob_, X.ent_type_) for X in tags])\n",
    "print(type(tags[0]))\n",
    "sentence_sets = [nlp(sentence) for sentence in df]\n",
    "with open('\"../../ner_sets.pkl', 'wb') as f:\n",
    "    pickle.dump(sentence_sets, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('texas', 'ORG'), ('university', 'ORG'), ('of', 'ORG')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python36\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "def jaccard_overlap(question_doc, sentence_doc):\n",
    "    \"\"\"\n",
    "        Compute the jaccard overlap between the set of question tokens and the set of answer sentence tokens\n",
    "        args:\n",
    "            A (set): Set of words in the question sentence\n",
    "            B (set): Set of words in the context_sentence\n",
    "        return:\n",
    "            jaccard_coefficient (float): The measure of jaccard overlap between the question and the context\n",
    "    \"\"\"\n",
    "    return question_doc.similarity(sentence_doc)\n",
    "\n",
    "def get_best_match_jaccard(question, df):\n",
    "    \"\"\"\n",
    "        Given a row from the training dataframe, output the index of the sentence with highest\n",
    "        jaccard overlap score\n",
    "        args:\n",
    "            df_row (pd.Dataframe): A row from the dataframe containing the training data.\n",
    "        return:\n",
    "            answer_sentence_index (int): The index of the sentence with the best jaccard overlap score\n",
    "    \"\"\"\n",
    "    \n",
    "    question = nlp(question)\n",
    "    question_token_set = set([(X.text, X.ent_type_) for X in question])\n",
    "    print(question_token_set)\n",
    "    index = [jaccard_overlap(question, sentence_set) for sentence_set in sentence_sets]\n",
    "    return np.argpartition(index, -5)[-5:]\n",
    "\n",
    "\n",
    "g = \"University of Texas\".lower()\n",
    "y = get_best_match_jaccard(g, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "827    Lydia Khalil discusses Trumpâ€™s response to COV...\n",
       "135    @ieykinr @khairulaming Eh, no la. Indonesia la...\n",
       "73     @the_hindu God bless your country!  @gauravbh ...\n",
       "450    CBS News: Boris Johnson puts U.K. on coronavir...\n",
       "83     @NRA @SteveGuest @RichardGrenell @kimguilfoyle...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "origin[y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python36\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<1177x448 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 18434 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tfidf_vectorizer = TfidfVectorizer(tokenizer=nltk.word_tokenize,\n",
    "                             stop_words=stop_words,\n",
    "                             ngram_range=(1,2),\n",
    "                             max_df=1.0,\n",
    "                             min_df=10)\n",
    "tfidf_vectorizer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf_answer(question, df, vectorizer, distance_func):\n",
    "    \"\"\"\n",
    "    Find the sentence in the context with the lowest distance from the question.\n",
    "    \n",
    "    args:\n",
    "        datapoint (pandas.core.series.Series): A row from the training dataframe\n",
    "        vectorizer (sklearn.feature_extraction.text.TfidfVectorizer): The tfidf_vectorizer model\n",
    "        distance_func (function): similarity function that takes in the question embedding and sentence embeddings and\n",
    "                                    returns a distance measurement between the two embeddings \n",
    "        \n",
    "    return:\n",
    "        int: The index of the context sentence with the lowest distance from the question\n",
    "    \"\"\"\n",
    "    question_vector = vectorizer.transform([question])\n",
    "    sentence_vectors = vectorizer.transform(df)\n",
    "    index = [distance_func(question_vector,v) for v in sentence_vectors]\n",
    "    return np.argmin(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "943"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_tfidf_answer(\"Donald Trump\",X,tfidf_vectorizer,cosine_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'donald trump lying murdering po http : co wcv5hhxcwq'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[943]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
